import pandas as pd
import pathway as pw
import datetime
from typing import Any
from pathway.stdlib.indexing.nearest_neighbors import BruteForceKnnFactory
from pathway.xpacks.llm import llms
from pathway.xpacks.llm.document_store import DocumentStore
from pathway.xpacks.llm.llms import LiteLLMChat
from pathway.xpacks.llm.embedders import SentenceTransformerEmbedder
import os
from dotenv import load_dotenv

from features.feature_tcp_flags import detect_abnormal_flags
from features.feature_ttl import analyze_ttl
from features.feature_small_packets import detect_small_packet_flow
from features.feature_sequence import analyze_sequence
from features.feature_encryption import get_encryption_label
from features.feature_flow_stats import compute_flow_stats

load_dotenv()

class PacketSchema(pw.Schema):
    timestamp: float
    protocols: str
    src_ip: str | None
    dst_ip: str | None
    src_port: str | None
    dst_port: str | None
    packet_size: str | None
    payload_len: str | None
    info: str | None
    tcp_seq: str | None
    tcp_flags_syn: str | None
    tcp_flags_ack: str | None
    tcp_flags_fin: str | None
    tcp_flags_rst: str | None
    tcp_flags_psh: str | None
    tcp_flags_urg: str | None
    tcp_retransmission: str | None
    tcp_window_size: str | None
    ttl_hop_limit: str | None
    fragmentation: str | None

# 1. Read Raw Packets
webserver = pw.io.http.PathwayWebserver(host="0.0.0.0", port=9000)
packets, _ = pw.io.http.rest_connector(
    webserver=webserver,
    schema=PacketSchema,
    autocommit_duration_ms=50
)
# pw.io.csv.write(packets, filename="docs/raw_packets.csv")

# Load Whitelist Configuration
WHITELIST = {"ips": [], "ports": []}
if os.path.exists("whitelist.json"):
    import json
    with open("whitelist.json", "r") as f:
        WHITELIST = json.load(f)

@pw.udf
def is_whitelisted(src_ip: str | None, dst_ip: str | None, src_port: str | None, dst_port: str | None) -> bool:
    # 1. Whitelist Localhost and Link-Local (IPv6)
    if src_ip and (src_ip == "::1" or src_ip.startswith("fe80:") or src_ip.startswith("ff02:")):
        return True
    if dst_ip and (dst_ip == "::1" or dst_ip.startswith("fe80:") or dst_ip.startswith("ff02:")):
        return True
        
    # 2. Whitelist Broadcast/Multicast
    if dst_ip and (dst_ip == "255.255.255.255" or dst_ip.startswith("224.")):
        return True

    # 3. Whitelist Common Safe Ports (DNS, mDNS, SSDP) if needed
    # (Optional: can be tunable)
    
    # Check against user-defined whitelist
    def check_ip_in_whitelist(ip):
        return ip in WHITELIST.get("ips", [])
    
    def check_port_in_whitelist(port):
        if not port: return False
        try:
            return int(port) in WHITELIST.get("ports", [])
        except ValueError:
            return False

    return (
        check_ip_in_whitelist(src_ip) or check_ip_in_whitelist(dst_ip) or 
        check_port_in_whitelist(src_port) or check_port_in_whitelist(dst_port)
    )

@pw.udf
def mask_if_whitelisted(value: Any, whitelisted: bool) -> Any:
    return None if whitelisted else value


@pw.udf
def has_flags(flags: list[str]) -> bool:
    return len(flags) > 0

@pw.udf
def check_anomaly(
    packet_count: int,
    mean_size: float,
    total_bytes: int,
    duration: float,
    dst_port: str | None
) -> str:
    reasons = []
    
    # Safe cast port
    dport = 0
    try:
        dport = int(dst_port) if dst_port else 0
    except ValueError:
        pass

    # 1. SYN Flood / Port Scan
    # Increase threshold to 500 for local high-traffic bursts
    rate = packet_count / max(duration, 0.001)
    if(packet_count > 10):
        reasons.append("Potential SYN Flood / Scan")
    if rate > 20 and mean_size < 100:
        reasons.append("Potential SYN Flood / Scan")

    # 2. Data Exfiltration (simplified proxy check)
    if total_bytes > 50000 and packet_count > 100:
         reasons.append(f"High Volume Transfer (Potential Exfiltration)")

    # 3. Beaconing (Skipped - requires IAT)

    # 4. Slow DoS (Relaxed)
    if duration > 2.0 and packet_count > 20:
        reasons.append("Potential Slow DoS Pattern")


    return "; ".join(reasons) if reasons else ""

@pw.udf
def safe_float_udf(x: str | None) -> float:
    try:
        if x is not None and str(x).strip():
            return float(x)
    except (ValueError, TypeError):
        pass
    return 0.0
@pw.udf
def to_bool_udf(val: str | None) -> bool:
    if val is None: return False
    return str(val).lower() in ("1", "true", "yes")

@pw.udf
def format_flow_id_udf(s: str | None, d: str | None, sp: str | None, dp: str | None) -> str:
    return f"{s or '?'}:{sp or '?' } -> {d or '?'}:{dp or '?'}"

@pw.udf
def get_last_packet_info_udf(infos: tuple) -> str:
    return str(infos[-1]) if infos else "None"

@pw.udf
def get_last_encryption_udf(encs: tuple) -> str:
    return str(encs[-1]) if encs else "Unknown"
@pw.udf(return_type=str)
def safe_flags_stub(*args) -> str:
    return "OK"

# 2. Add individual packet features
packets = packets.select(
    *pw.this,
    # abnormal_flags = pw.apply(
    #     detect_abnormal_flags,
    #     pw.this.protocols,
    #     pw.this.tcp_flags_syn, pw.this.tcp_flags_ack, pw.this.tcp_flags_fin, 
    #     pw.this.tcp_flags_rst, pw.this.tcp_flags_psh, pw.this.tcp_flags_urg
    # ),
    abnormal_flags = "OK",
    is_encrypted="Unknown"
    # is_encrypted = pw.apply(get_encryption_label, pw.this.protocols, pw.this.dst_port)
)
# pw.io.csv.write(packets, filename="docs/raw_packets.csv")


# 2. Canonical Flow Key (Bidirectional)
@pw.udf
def canonical_key(sip, dip, sport, dport, proto):
    if sip is None or dip is None:
        return (sip, dip, sport, dport, proto)

    sip = sip.split(",")[0]
    dip = dip.split(",")[0]
    sport = sport or "0"
    dport = dport or "0"

    forward = (sip, sport, dip, dport, proto)
    reverse = (dip, dport, sip, sport, proto)

    return min(forward, reverse)

@pw.udf(return_type=int)
def to_int_udf(x: str | None) -> int:
    try:
        return int(x) if x else 0
    except:
        return 0


packets_with_key = packets.select(
    *pw.this,
    flow_key = canonical_key(pw.this.src_ip, pw.this.dst_ip, pw.this.src_port, pw.this.dst_port, pw.this.protocols),
    # Ensure types for UDF
    ts_float = pw.this.timestamp,
    size_int = to_int_udf(pw.this.packet_size),
    seq_str = pw.apply(lambda x: str(x) if x else "0", pw.this.tcp_seq)
)


# 3. Window aggregation with Advanced Stats
# 3. Window aggregation with Advanced Stats
flow_stats = packets_with_key.groupby(pw.this.flow_key).windowby(
    pw.this.timestamp,
    window=pw.temporal.sliding(hop=0.5, duration=5.0),
).reduce(
    packet_count=pw.reducers.count(),
    total_bytes=pw.reducers.sum(pw.this.size_int),
    mean_size=pw.reducers.avg(pw.this.size_int),
    min_time=pw.reducers.min(pw.this.timestamp),
    max_time=pw.reducers.max(pw.this.timestamp),
    event_time=pw.reducers.max(pw.this.timestamp),

    src_ip=pw.reducers.max(pw.this.src_ip),
    dst_ip=pw.reducers.max(pw.this.dst_ip),
    src_port=pw.reducers.max(pw.this.src_port),
    dst_port=pw.reducers.max(pw.this.dst_port),
    is_encrypted=pw.reducers.max(pw.this.is_encrypted),
)


# Unpack logic
flow_features = flow_stats.select(
    flow_id=format_flow_id_udf(
        pw.this.src_ip,
        pw.this.dst_ip,
        pw.this.src_port,
        pw.this.dst_port
    ),
    duration=pw.this.max_time - pw.this.min_time+0.001,
    packet_count=pw.this.packet_count,
    total_bytes=pw.this.total_bytes,
    mean_size=pw.this.mean_size,
    event_time=pw.this.event_time,

    src_ip=pw.this.src_ip,
    dst_ip=pw.this.dst_ip,
    src_port=pw.this.src_port,
    dst_port=pw.this.dst_port,
    is_encrypted=pw.this.is_encrypted
)

@pw.udf(return_type=float)
def anomaly_score(packet_count, mean_size, total_bytes, duration):
    rate = packet_count / max(duration, 0.001)

    score = 0.0
    if packet_count > 5:
        return 1.0
    if packet_count > 50 and mean_size < 120:
        score += 0.4

    if total_bytes > 2000:
        score += 0.3

    if duration > 5:
        score += 0.3

    return min(score, 1.0)


@pw.udf(return_type=float)
def confidence(packet_count, duration):
    if duration == 0:
        return 0.0
    density = packet_count / duration
    if density > 50:
        return 0.9
    if density > 10:
        return 0.7
    return 0.5


@pw.udf
def filter_flags(flags: tuple[str | None, ...]) -> list[str]:
    return [f for f in flags if f is not None]

@pw.udf
def count_flags(flags: list[str] | None) -> int:
    if flags is None:
        return 0
    # Pathway might pass tuple if internal representation changes, handle gracefully
    return len(flags)

# 4. Apply Behavioral Analysis to Flows
flows_with_whitelist = flow_features.select(
    *pw.this,
    whitelisted = is_whitelisted(pw.this.src_ip, pw.this.dst_ip, pw.this.src_port, pw.this.dst_port)
)
@pw.udf(return_type=float)
def mask_score(score: float, whitelisted: bool) -> float:
    return 0.0 if whitelisted else score

flow_analysis = flows_with_whitelist.select(
    *pw.this,
    raw_score=anomaly_score(
        pw.this.packet_count,
        pw.this.mean_size,
        pw.this.total_bytes,
        pw.this.duration
    ),
    anomaly_reason=check_anomaly(
        pw.this.packet_count,
        pw.this.mean_size,
        pw.this.total_bytes,
        pw.this.duration,
        pw.this.dst_port
    )
).select(
    *pw.this,
    anomaly_score=mask_score(
        pw.this.raw_score,
        pw.this.whitelisted
    ),
    confidence=confidence(
        pw.this.packet_count,
        pw.this.duration
    )
)
debug_8080 = flow_analysis.filter(
    pw.this.dst_port == "8080"
)
# pw.io.csv.write(
#     debug_8080.select(
#         pw.this.flow_id,
#         pw.this.packet_count,
#         pw.this.total_bytes,
#         pw.this.mean_size,
#         pw.this.duration,
#         pw.this.raw_score,
#         pw.this.anomaly_score,
#         pw.this.anomaly_reason,
#         pw.this.confidence,
#         pw.this.event_time
#     ),
#     filename="docs/debug_8080.csv"
# )




# 5. Push to Web Dashboard (Rate-Limited Pulse)
# We window the analysis to send updates every 2s for UI stability
flow_pulse = flow_analysis.windowby(
    pw.this.event_time,
    window=pw.temporal.tumbling(duration=2.0),
    instance=pw.this.flow_id
).reduce(
    flow_id=pw.reducers.max(pw.this.flow_id),
    anomaly_score=pw.reducers.max(pw.this.anomaly_score),
    anomaly_reason=pw.reducers.max(pw.this.anomaly_reason),
    last_packet_info=pw.reducers.max(pw.this.anomaly_reason),
    confidence=pw.reducers.max(pw.this.confidence),
    packet_count=pw.reducers.max(pw.this.packet_count),
    total_bytes=pw.reducers.max(pw.this.total_bytes),
    duration=pw.reducers.max(pw.this.duration),
    event_time=pw.reducers.max(pw.this.event_time),
    # Aliases for Frontend
    flow=pw.reducers.max(pw.this.flow_id), 
    last_packet_time=pw.reducers.max(pw.this.event_time),
    encryption=pw.reducers.max(pw.this.is_encrypted),


)


# Filter for anomalies only (Shared logic)
# anomalous_pulse = flow_pulse.filter(
#     pw.this.anomaly_score > 0.5
# )
anomalous_pulse = flow_pulse

pw.io.http.write(
    anomalous_pulse,
    url="http://localhost:8000/api/update/",
    method="POST",
    headers={"Content-Type": "application/json"}
)

# Debug: Write to CSV for manual check
# pw.io.csv.write(flow_pulse, filename="docs/debug_flows.csv")

pw.io.csv.write(
    anomalous_pulse,
    filename="docs/anomalies.csv"
)

# 6. Format for LLM Indexing (DocumentStore)
# 6. Format for LLM Indexing (DocumentStore)
@pw.udf
def format_doc_udf(flow_id, score, confidence, packets, bytes_sent, duration) -> bytes:
    return (
        f"Flow: {flow_id} | "
        f"Score: {score:.2f} | "
        f"Confidence: {confidence:.2f} | "
        f"Packets: {packets} | "
        f"Bytes: {bytes_sent} | "
        f"Duration: {duration:.2f}s"
    ).encode("utf-8")

live_docs = anomalous_pulse.select(
    data=format_doc_udf(
        pw.this.flow_id,
        pw.this.anomaly_score,
        pw.this.confidence,
        pw.this.packet_count,
        pw.this.total_bytes,
        pw.this.duration,
    ),
    _metadata=pw.apply(
        lambda _: {
            "modified_at": 0,
            "seen_at": 0,
            "path": "live_stream"
        },
        pw.this.flow_id
    )
).select(
    pw.this.data,
    pw.this._metadata
)

@pw.udf
def ensure_bytes(x) -> bytes:
    if isinstance(x, bytes):
        return x
    return str(x).encode("utf-8")

# FIX: Add static document to prevent Empty Index Panic during startup
static_df = pd.DataFrame([
    {
        "data": b"Sentinel System initialized. Monitoring network traffic...",
        "_metadata": {"modified_at": 0, "seen_at": 0, "path": "static_init"}
    }
])

@pw.udf
def ensure_metadata(meta) -> dict:
    MAX_I64 = 9223372036854775807
    try:
        ts = int(meta.get("modified_at", 0))
    except:
        ts = 0

    ts = max(0, min(ts, MAX_I64))

    return {
        "modified_at": ts,
        "seen_at": ts,
        "path": str(meta.get("path", "static_init"))
    }

static_docs = pw.debug.table_from_pandas(static_df).select(
    data=ensure_bytes(pw.this.data),
    _metadata=ensure_metadata(pw.this._metadata)
)

# --- RAG / VECTOR STORE DISABLED FOR STABILITY ---
# The DocumentStore is causing persistent panic (TryFromIntError).
# We are temporarily bypassing it to allow the main Anomaly Detection system to run.

# Indexing
embedder = SentenceTransformerEmbedder(model="all-MiniLM-L6-v2")
# retriever_factory = BruteForceKnnFactory(embedder=embedder)
# document_store = DocumentStore(docs=analyzed_docs, retriever_factory=retriever_factory)

# Webserver & Queries
query_server = pw.io.http.PathwayWebserver(host="0.0.0.0", port=8011)


class QuerySchema(pw.Schema):
    messages: str

queries, writer = pw.io.http.rest_connector(
    webserver=query_server,
    schema=QuerySchema,
    autocommit_duration_ms=1000,
    delete_completed_queries=True
)

# Process Queries
queries_processed = queries.select(
    query = pw.this.messages,
)

# BYPASS: Join with empty result instead of calling document_store.retrieve_query
queries_context = queries_processed.select(
    *pw.this,
    result = pw.apply(lambda x: [], pw.this.query) 
)

@pw.udf
def build_prompts_udf(documents, query) -> str:
    # Dummy prompt builder since RAG is disabled
    return f"Analyze this network status based on general knowledge.\nUser Question: {query}\n(Note: Live context retrieval is temporarily disabled due to system instability.)"

prompts = queries_context.select(
     prompt_text=build_prompts_udf(pw.this.result, pw.this.query)
)

model = LiteLLMChat(
    model="openrouter/arcee-ai/trinity-large-preview:free",
    api_key=os.environ.get("OPENROUTER_API_KEY"),
    base_url="https://openrouter.ai/api/v1",
    retry_strategy=pw.udfs.FixedDelayRetryStrategy(2,3)
)

responses = prompts.select(
    result = pw.apply(lambda r: str(r), model(llms.prompt_chat_single_qa(pw.this.prompt_text)))
)

writer(responses)
pw.run()